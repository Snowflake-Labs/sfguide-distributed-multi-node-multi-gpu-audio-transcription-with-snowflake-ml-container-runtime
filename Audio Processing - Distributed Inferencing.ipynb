{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0d9736dc-2c27-4e68-bdae-ebb724531507",
   "metadata": {
    "collapsed": false,
    "name": "intro"
   },
   "source": [
    "# Distributed Multi-Node, Multi-GPU Audio Transcription in ML Container Runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3775908f-ca36-4846-8f38-5adca39217f2",
   "metadata": {
    "language": "python",
    "name": "import_libs"
   },
   "outputs": [],
   "source": [
    "# Import python packages\n",
    "import streamlit as st\n",
    "import pandas as pd\n",
    "import torch\n",
    "# We can also use Snowpark for our analyses!\n",
    "from typing import Dict\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import shutil\n",
    "from snowflake.snowpark.context import get_active_session\n",
    "from snowflake.ml.ray.datasource import SFStageBinaryFileDataSource\n",
    "from transformers import AutoModelForSpeechSeq2Seq, AutoProcessor, pipeline\n",
    "from snowflake.ml.runtime_cluster import scale_cluster, get_nodes\n",
    "from snowflake.ml.ray.datasink import SnowflakeTableDatasink\n",
    "import ray\n",
    "import subprocess\n",
    "import logging\n",
    "session = get_active_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a55da4f5-449a-4c69-9ef0-3a5f70421e17",
   "metadata": {
    "collapsed": false,
    "name": "intro_see_starter_nodes"
   },
   "source": [
    "### Start with one node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b23ce5cd-c2b0-41fa-bcec-4a77fda4cf0e",
   "metadata": {
    "language": "python",
    "name": "initialize_ray_and_start"
   },
   "outputs": [],
   "source": [
    "ray.init(ignore_reinit_error=True)\n",
    "num_nodes = len([node for node in ray.nodes() if node[\"Alive\"]==True])\n",
    "print(num_nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df11d409-2cb0-4107-902b-502699d54fe5",
   "metadata": {
    "collapsed": false,
    "name": "intro_scale_to_5_nodes"
   },
   "source": [
    "### Scale up to 5 nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efebb414-e8f2-46ee-adda-173da0ced783",
   "metadata": {
    "codeCollapsed": false,
    "language": "python",
    "name": "scale_to_5_nodes"
   },
   "outputs": [],
   "source": [
    "# Asynchronous scaling - function returns immediately after request is accepted\n",
    "scale_cluster(5, is_async=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c66019ea-840e-490b-bc91-30112590595f",
   "metadata": {
    "collapsed": false,
    "name": "intro_control_ray_logging"
   },
   "source": [
    "### Control ray logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa5778d7-1e10-4831-9757-1410fdc7a383",
   "metadata": {
    "language": "python",
    "name": "control_ray_logging"
   },
   "outputs": [],
   "source": [
    "def configure_ray_logger() -> None:\n",
    "    #Configure Ray logging\n",
    "    ray_logger = logging.getLogger(\"ray\")\n",
    "    ray_logger.setLevel(logging.CRITICAL)\n",
    "\n",
    "    data_logger = logging.getLogger(\"ray.data\")\n",
    "    data_logger.setLevel(logging.CRITICAL)\n",
    "\n",
    "    #Configure root logger\n",
    "    logger = logging.getLogger()\n",
    "    logger.setLevel(logging.CRITICAL)\n",
    "\n",
    "    #Configure Ray's data context\n",
    "    context = ray.data.DataContext.get_current()\n",
    "    context.execution_options.verbose_progress = False\n",
    "    context.enable_operator_progress_bars = False\n",
    "\n",
    "configure_ray_logger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bee4cc4-a6eb-443a-a34c-66fddbfcc151",
   "metadata": {
    "codeCollapsed": false,
    "language": "python",
    "name": "install_ffmpeg_each_node"
   },
   "outputs": [],
   "source": [
    "! ffmpeg -version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9abca883-cf62-4603-ad55-3273b4c0806b",
   "metadata": {
    "codeCollapsed": false,
    "language": "python",
    "name": "print_gpus_in_ray_cluster"
   },
   "outputs": [],
   "source": [
    "print(int(ray.cluster_resources()['GPU']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "416d5937-05b5-473a-b3b3-b58d3e5c998a",
   "metadata": {
    "collapsed": false,
    "name": "intro_see_audio_files"
   },
   "source": [
    "### See audio files in snowflake stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "938569bd-67ca-4657-b922-b126aef46f91",
   "metadata": {
    "codeCollapsed": false,
    "language": "sql",
    "name": "see_audio_files"
   },
   "outputs": [],
   "source": [
    "ls @AUDIO_FILES_STAGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f5905b6-5665-4c25-a44f-747b3b86fd7f",
   "metadata": {
    "codeCollapsed": false,
    "language": "python",
    "name": "get_ray_dataset_using_snow_apis"
   },
   "outputs": [],
   "source": [
    "audio_source = SFStageBinaryFileDataSource(\n",
    "    stage_location = \"@AUDIO_FILES_STAGE/\",\n",
    "    database = session.get_current_database(),\n",
    "    schema = session.get_current_schema(),\n",
    "    file_pattern = \"*.flac\"\n",
    ")\n",
    "\n",
    "# Load audio files into a ray dataset\n",
    "audio_dataset = ray.data.read_datasource(audio_source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7da5a52-bcd4-4109-8a33-00bd20c19504",
   "metadata": {
    "codeCollapsed": false,
    "language": "python",
    "name": "see_sample_audio_files"
   },
   "outputs": [],
   "source": [
    "audio_dataset.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ae5462d-e0e4-4396-b383-1c7e1f7645b9",
   "metadata": {
    "codeCollapsed": false,
    "language": "python",
    "name": "see_count_of_audio_files"
   },
   "outputs": [],
   "source": [
    "audio_dataset.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12dcd965-ea6d-4850-9786-44c274495bd3",
   "metadata": {
    "collapsed": false,
    "name": "intro_get_whisper_model"
   },
   "source": [
    "### Get whisper model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1605a95-9fc0-4511-ae2b-7343208a242a",
   "metadata": {
    "codeCollapsed": false,
    "language": "python",
    "name": "set_model_params"
   },
   "outputs": [],
   "source": [
    "model_id = \"openai/whisper-large-v3\"\n",
    "batch_size = 30\n",
    "is_cuda_available = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if is_cuda_available else \"cpu\")\n",
    "torch_dtype = torch.float16 if is_cuda_available else torch.float32\n",
    "print(device)\n",
    "print(torch_dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c19ddbbe-3cdc-4964-91ef-8c316dab899e",
   "metadata": {
    "collapsed": false,
    "name": "intro_distributed_inferencing"
   },
   "source": [
    "### Distributed inferencing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "027b9201-d3e2-492b-b525-b78335d57a4f",
   "metadata": {
    "language": "python",
    "name": "audio_transcription_class"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tempfile\n",
    "import os\n",
    "\n",
    "class TranscribeAudioUpdated:\n",
    "    def __init__(self):\n",
    "        # initialize model here so that model can be put into correct GPU/node\n",
    "        model = AutoModelForSpeechSeq2Seq.from_pretrained(\n",
    "            model_id, torch_dtype=torch_dtype, low_cpu_mem_usage=True, use_safetensors=True\n",
    "        )\n",
    "        model.to(device)\n",
    "        processor = AutoProcessor.from_pretrained(model_id)\n",
    "        self.pipe = pipeline(\n",
    "            \"automatic-speech-recognition\",\n",
    "            model=model,\n",
    "            tokenizer=processor.tokenizer,\n",
    "            feature_extractor=processor.feature_extractor,\n",
    "            max_new_tokens=128,\n",
    "            chunk_length_s=30,\n",
    "            batch_size=batch_size,\n",
    "            return_timestamps=True,\n",
    "            torch_dtype=torch_dtype,\n",
    "            device=device,\n",
    "            generate_kwargs={\"language\": \"english\"}\n",
    "        )\n",
    "\n",
    "    def __call__(self, batch: pd.DataFrame) -> pd.DataFrame:\n",
    "        temp_files = []\n",
    "        try:\n",
    "            # Write each binary to a temporary file.\n",
    "            for binary_content in batch[\"file_binary\"]:\n",
    "                # Use an appropriate suffix (e.g., .wav or .flac) based on your audio format.\n",
    "                tmp_file = tempfile.NamedTemporaryFile(delete=False, suffix=\".flac\")\n",
    "                tmp_file.write(binary_content)\n",
    "                tmp_file.close()\n",
    "                temp_files.append(tmp_file.name)\n",
    "            \n",
    "            # Use the temporary file paths for inference.\n",
    "            predictions = self.pipe(temp_files)\n",
    "            assert len(predictions) == len(batch)\n",
    "            outputs = [str(generated_audio[\"text\"]).strip() for generated_audio in predictions]\n",
    "            batch['outputs'] = outputs\n",
    "            batch.drop(columns=['file_binary'], inplace=True)\n",
    "        finally:\n",
    "            # Clean up temporary files.\n",
    "            for file_path in temp_files:\n",
    "                try:\n",
    "                    os.remove(file_path)\n",
    "                except OSError:\n",
    "                    pass\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2b1dec8-2eec-4874-93b2-7771b973f0f4",
   "metadata": {
    "codeCollapsed": false,
    "language": "python",
    "name": "map_batches_of_audio_files"
   },
   "outputs": [],
   "source": [
    "transcribed_ds = audio_dataset.map_batches(TranscribeAudioUpdated,\n",
    "        batch_size=batch_size,\n",
    "        batch_format='pandas',\n",
    "        concurrency=5,\n",
    "        num_gpus=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dff09d59-99ab-4ca7-9d27-bb17133dcb59",
   "metadata": {
    "language": "sql",
    "name": "drop_results_table_if_exists"
   },
   "outputs": [],
   "source": [
    "drop table if exists WHISPER_DEMO_OUTPUT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99b998b9-ea51-4536-9da2-9539f3615f4d",
   "metadata": {
    "language": "python",
    "name": "output_to_a_snowflake_table"
   },
   "outputs": [],
   "source": [
    "datasink = SnowflakeTableDatasink(\n",
    "    table_name=\"WHISPER_DEMO_OUTPUT\",\n",
    "    database=session.get_current_database(),\n",
    "    schema=session.get_current_schema(),\n",
    "    auto_create_table=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e10e0495-e8aa-483e-8bff-d7797e099afa",
   "metadata": {
    "codeCollapsed": false,
    "language": "python",
    "name": "write_transcriptions_to_snowflake"
   },
   "outputs": [],
   "source": [
    "transcribed_ds.write_datasink(datasink)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73cf7b38-3fb7-4dad-ad5f-2415f6eb5788",
   "metadata": {
    "codeCollapsed": false,
    "language": "python",
    "name": "see_results"
   },
   "outputs": [],
   "source": [
    "session.table(\"WHISPER_DEMO_OUTPUT\").show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Streamlit Notebook",
   "name": "streamlit"
  },
  "lastEditStatus": {
   "authorEmail": "Puneet.Lakhanpal@Snowflake.com",
   "authorId": "233194668061",
   "authorName": "PLAKHANPAL",
   "lastEditTime": 1741240132087,
   "notebookId": "obbbvn35fc7owxmeik3m",
   "sessionId": "5e774511-e974-4eb8-a0f9-383782520a5d"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
